{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\gutenberg.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gutenberg files :  ['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    }
   ],
   "source": [
    "gb = nltk.corpus.gutenberg\n",
    "print(\"Gutenberg files : \", gb.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23140\n",
      "['[', 'The', 'Tragedie', 'of', 'Macbeth', 'by', 'William', 'Shakespeare', '1603', ']']\n"
     ]
    }
   ],
   "source": [
    "macbeth = nltk.corpus.gutenberg.words('shakespeare-macbeth.txt')\n",
    "print(len(macbeth))\n",
    "print(macbeth[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['[',\n",
       "  'The',\n",
       "  'Tragedie',\n",
       "  'of',\n",
       "  'Macbeth',\n",
       "  'by',\n",
       "  'William',\n",
       "  'Shakespeare',\n",
       "  '1603',\n",
       "  ']'],\n",
       " ['Actus', 'Primus', '.'],\n",
       " ['Scoena', 'Prima', '.'],\n",
       " ['Thunder', 'and', 'Lightning', '.'],\n",
       " ['Enter', 'three', 'Witches', '.']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macbeth_sents = nltk.corpus.gutenberg.sents('shakespeare-macbeth.txt')\n",
    "macbeth_sents[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 3 of 3 matches:\n",
      "nts with Dishes and Seruice ouer the Stage . Then enter Macbeth Macb . If it we\n",
      "with mans Act , Threatens his bloody Stage : byth ' Clock ' tis Day , And yet d\n",
      " struts and frets his houre vpon the Stage , And then is heard no more . It is \n",
      "None \n",
      "\n",
      "the_. bloody_: the_,\n",
      "None \n",
      "\n",
      "day time face warre ayre king bleeding man reuolt serieant like\n",
      "knowledge broyle shew head spring heeles hare thane skie\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "text = nltk.Text(macbeth)\n",
    "print(text.concordance('Stage'),'\\n')\n",
    "print(text.common_contexts(['Stage']),'\\n')\n",
    "print(text.similar('Stage'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 1962),\n",
       " ('.', 1235),\n",
       " (\"'\", 637),\n",
       " ('the', 531),\n",
       " (':', 477),\n",
       " ('and', 376),\n",
       " ('I', 333),\n",
       " ('of', 315),\n",
       " ('to', 311),\n",
       " ('?', 241)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd = nltk.FreqDist(macbeth)\n",
    "fd.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['during',\n",
       " 'wouldn',\n",
       " \"didn't\",\n",
       " 'now',\n",
       " 'before',\n",
       " 's',\n",
       " 'a',\n",
       " 'aren',\n",
       " \"you're\",\n",
       " \"shouldn't\"]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sw = set(nltk.corpus.stopwords.words('english'))\n",
    "print(len(sw))\n",
    "list(sw)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14946\n",
      "[(',', 1962), ('.', 1235), (\"'\", 637), (':', 477), ('?', 241), ('Macb', 137), ('haue', 117), ('-', 100), ('Enter', 80), ('thou', 63)]\n"
     ]
    }
   ],
   "source": [
    "macbeth_filtered = [w for w in macbeth if w.lower() not in sw]\n",
    "print(len(macbeth_filtered))\n",
    "\n",
    "fd = nltk.FreqDist(macbeth_filtered)\n",
    "print(fd.most_common(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('macb', 137),\n",
       " ('haue', 122),\n",
       " ('thou', 90),\n",
       " ('enter', 81),\n",
       " ('shall', 68),\n",
       " ('macbeth', 62),\n",
       " ('vpon', 62),\n",
       " ('thee', 61),\n",
       " ('macd', 58),\n",
       " ('vs', 57)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "punctuation = set(string.punctuation)\n",
    "macbeth_filtered2 = [w.lower() for w in macbeth if w.lower() not in sw and w.lower() not in punctuation]\n",
    "fd = nltk.FreqDist(macbeth_filtered2)\n",
    "fd.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Assassination', 'Chamberlaines', 'Distinguishes', 'Gallowgrosses', 'Metaphysicall', 'Northumberland', 'Voluptuousnesse', 'commendations', 'multitudinous', 'supernaturall', 'vnaccompanied']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Auaricious',\n",
       " 'Gracious',\n",
       " 'Industrious',\n",
       " 'Iudicious',\n",
       " 'Luxurious',\n",
       " 'Malicious',\n",
       " 'Obliuious',\n",
       " 'Pious',\n",
       " 'Rebellious',\n",
       " 'compunctious',\n",
       " 'furious',\n",
       " 'gracious',\n",
       " 'pernicious',\n",
       " 'pernitious',\n",
       " 'pious',\n",
       " 'precious',\n",
       " 'rebellious',\n",
       " 'sacrilegious',\n",
       " 'serious',\n",
       " 'spacious',\n",
       " 'tedious']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_words = [w for w in macbeth if len(w)> 12]\n",
    "print(sorted(long_words))\n",
    "\n",
    "\n",
    "ious_words = [w for w in macbeth if 'ious' in w]\n",
    "ious_words = set(ious_words)\n",
    "sorted(ious_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('enter', 'macbeth'), 16),\n",
       " (('exeunt', 'scena'), 15),\n",
       " (('thane', 'cawdor'), 13),\n",
       " (('knock', 'knock'), 10),\n",
       " (('st', 'thou'), 9),\n",
       " (('thou', 'art'), 9),\n",
       " (('lord', 'macb'), 9),\n",
       " (('haue', 'done'), 8),\n",
       " (('macb', 'haue'), 8),\n",
       " (('good', 'lord'), 8),\n",
       " (('let', 'vs'), 7),\n",
       " (('enter', 'lady'), 7),\n",
       " (('wee', 'l'), 7),\n",
       " (('would', 'st'), 6),\n",
       " (('macbeth', 'macb'), 6)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bgrms = nltk.FreqDist(nltk.bigrams(macbeth_filtered2))\n",
    "bgrms.most_common(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('knock', 'knock', 'knock'), 6),\n",
       " (('enter', 'macbeth', 'macb'), 5),\n",
       " (('enter', 'three', 'witches'), 4),\n",
       " (('exeunt', 'scena', 'secunda'), 4),\n",
       " (('good', 'lord', 'macb'), 4),\n",
       " (('three', 'witches', '1'), 3),\n",
       " (('exeunt', 'scena', 'tertia'), 3),\n",
       " (('thunder', 'enter', 'three'), 3),\n",
       " (('exeunt', 'scena', 'quarta'), 3),\n",
       " (('scena', 'prima', 'enter'), 3)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgrms = nltk.FreqDist(nltk.trigrams (macbeth_filtered2))\n",
    "tgrms.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ufeffThe Project Gutenberg eBook of Crime and Punishment, by Fyodor Dostoevsky\\r'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib import request\n",
    "url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n",
    "response = request.urlopen(url)\n",
    "raw = response.read().decode('utf8')\n",
    "raw[:75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\ufeffThe',\n",
       " 'Project',\n",
       " 'Gutenberg',\n",
       " 'eBook',\n",
       " 'of',\n",
       " 'Crime',\n",
       " 'and',\n",
       " 'Punishment',\n",
       " ',',\n",
       " 'by',\n",
       " 'Fyodor',\n",
       " 'Dostoevsky']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize (raw)\n",
    "webtext = nltk.Text (tokens)\n",
    "webtext[:12]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!doctype html public \"-//W3C//DTD HTML 4.0 Transitional//EN\" \"http://www.w3.org/TR/REC-html40/loose.dtd\">\\r\\n<html>\\r\\n<hea'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"http://news.bbc.co.uk/2/hi/health/2284783.stm\"\n",
    "html = request.urlopen(url).read().decode('utf8')\n",
    "html[:120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Text: BBC NEWS | Health | Blondes 'to die...>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "raw = BeautifulSoup(html, \"lxml\").get_text()\n",
    "tokens = nltk.word_tokenize(raw)\n",
    "text = nltk.Text(tokens)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('movie_reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "reviews = nltk.corpus.movie_reviews\n",
    "documents = [(list(reviews.words(fileid)), category) for category in reviews.categories() for fileid in reviews.fileids(category)]\n",
    "random.shuffle(documents)\n",
    "first_review = ' '.join(documents[0][0])\n",
    "all_words = nltk.FreqDist(w.lower() for w in reviews.words())\n",
    "word_features = list(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_features(document, word_features):\n",
    " document_words = set(document)\n",
    " features = {}\n",
    " for word in word_features:\n",
    "    features['{}'.format(word)] = (word in document_words)\n",
    " return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featuresets = [(document_features(d,word_features), c) for (d,c) in documents]\n",
    "len(featuresets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.744\n",
      "Most Informative Features\n",
      "              nomination = True              pos : neg    =      9.1 : 1.0\n",
      "                   laura = True              pos : neg    =      8.5 : 1.0\n",
      "                  lonely = True              pos : neg    =      8.5 : 1.0\n",
      "                    fits = True              pos : neg    =      7.8 : 1.0\n",
      "                 foreign = True              pos : neg    =      7.8 : 1.0\n",
      "             magnificent = True              pos : neg    =      7.8 : 1.0\n",
      "                   notch = True              pos : neg    =      7.8 : 1.0\n",
      "                    bore = True              neg : pos    =      7.5 : 1.0\n",
      "                 breasts = True              neg : pos    =      7.5 : 1.0\n",
      "                   waste = True              neg : pos    =      7.5 : 1.0\n"
     ]
    }
   ],
   "source": [
    "train_set, test_set = featuresets[1500:], featuresets[:500]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "train_set, est_set = featuresets[1500:], featuresets[:500]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))\n",
    "classifier.show_most_informative_features(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
